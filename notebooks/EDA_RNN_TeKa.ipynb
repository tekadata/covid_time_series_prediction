{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac72ae-9be5-43d4-87d3-b2319a0f757b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Time Series COVID-19 Cases Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a246bb7-0cdf-4939-a4e9-870f871706f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd5297-79e4-4864-bb68-8deb37dbc70a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dependencies importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c99354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload imported module every time a jupyter cell is executed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de425903-9365-4407-8ae8-9b20a6e41dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import requests\n",
    "import pandas_profiling\n",
    "from typing import overload\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop \n",
    "from covid_time_series_prediction.ml_logic import preprocessor\n",
    "# from ml_logic.country_data import country_output\n",
    "from covid_time_series_prediction.ml_logic.preprocessor import train_test_set, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332849e5-8682-4079-8266-8a22c70d7d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee22a40-a282-4e4f-bf18-ce8a7633b59c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e9b20-7339-486d-8ebc-21b0394c5e5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### By country over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c36f91-5db2-4aa7-b598-45ae52f2dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_time_series(feature='stringency', start_date='2020-02-14', end_date='2021-02-14'):\n",
    "    \"\"\"\n",
    "    Get stringency time series for each countries requesting API.\n",
    "    Returns json dict with TS between start_date and end_date like 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    url = f'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/{feature}/date-range/{start_date}/{end_date}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f6689-5b7f-42e3-891b-57367b4658e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_time_series_api = fetch_time_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bad4c-3c99-4620-aead-b7951a993db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, [c for c in v if c == 'VNM'])  for k, v in countries_time_series_api.items()  if k == 'countries' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59acb5e4-2aee-4aa5-be96-ac8af9d3a96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[([([([(vee)  for kaaa, veee  in vee.items() if kaaa in ['date_value', 'confirmed']  ])  for kaa, vee  in ve.items() if kaa =='VNM'   ])  for ka, ve  in v.items() ])  for k, v in countries_time_series_api.items() if k=='data'   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb51b76-85af-4077-8db0-4b74529104a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[(k, [(ka, [(kaa, vee)  for kaa, vee  in ve.items() if kaa =='USA'   ])  for ka, ve  in v.items() ])  for k, v in countries_time_series_api.items() if k=='data'   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b518b601-27e8-4411-9b10-165c94c89c3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Country data for a specific day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e858c-c9e9-4d78-9d94-804e06fbf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(country='USA', date='2020-08-14'):\n",
    "    \"\"\"\n",
    "    Get stringency data for one country {ALPHA-3} requesting API.\n",
    "    Returns json dict with data for country like 'AAA' and specific date and like 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    url = f'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/actions/{country}/{date}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debbac8-54a8-482a-b339-d5fa249c4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data_api = fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c13eee-1681-40ca-b0d2-2ab21437191f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[[';'.join([str(kk) for kk, vv in d.items()]) for i, d in enumerate(v) if type(d) == dict and i == 0] for v in country_data_api.values()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81f0b8-9fc7-4e38-8e24-99408857cb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[[';'.join([str(vv) for kk, vv in d.items()]) for d in v if type(d) == dict] for v in country_data_api.values()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869ee50-edb3-423d-8a75-f05af3f32afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [';'.join([str(vv) for vv in v]) for v in country_data_api.values()][-1]\n",
    "[';'.join([str(kk) for kk in v]) for k, v in country_data_api.items()][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad4771-e464-4eb1-86f5-c5c480cea8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1591b-31c5-4a38-b0fb-4bfaec0924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data project directory\n",
    "data_dir = '../data/raw_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7c791-f898-4106-b9a9-f1221fcfb1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Read URL**, **Get CSV files** and **store CSV in local**  *(optional do it at begining or to refresh CSV data)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318d9af-8148-4bbc-9dff-7359600f778d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **get_database_to_csv()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42cd6c-5efd-4cad-b74b-d134e2653dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_to_csv(url, csv_list, path='', db_grid=[]) -> list:\n",
    "    \"\"\"\n",
    "    function that take in parameter:\n",
    "     - a root URL (string) to get the CSV data,\n",
    "     - a list of CSV files,\n",
    "     - a path (string) to store CSV in local,\n",
    "     - a grid (list of list) to add in the CSV filename, URL, local path.     \n",
    "    and returns the gird updated with the CSVs of the list\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ### Create a database grid (list of list) with all CSVs and associated URLs\n",
    "    # print('db_grid', db_grid)\n",
    "    #### Data project directory (if empty do not store CSV in local)\n",
    "    # print('path', path)\n",
    "    ### Website CSV datasets URL\n",
    "    # print('url', url)\n",
    "    #### List of CSVs of Website to retrieve\n",
    "    # print('csv_list', csv_list)\n",
    "\n",
    "    #### Length of grid aka number of CSVs already stored in grid\n",
    "    len_grid = len(db_grid)\n",
    "\n",
    "    for l in range(len(csv_list)):\n",
    "        sub_list = []       \n",
    "        sub_list.append(csv_list[l]) ## 1st pos¬∞: CSV filename\n",
    "        sub_list.append(url + csv_list[l]) ## 2nd pos¬∞: URL + CSV\n",
    "        if len(data_dir) > 0: ## store CSV in local\n",
    "            sub_list.append(data_dir + csv_list[l]) ## 3rd pos¬∞: local data path + CSV\n",
    "            !curl -L \"{url + csv_list[l]}\" > {data_dir + csv_list[l]} ## curl <URL>/<CSV> => <path>\n",
    "        # print('sub_list', sub_list)\n",
    "        db_grid.append(sub_list)\n",
    "\n",
    "    ### Return a database grid (list of list) with all CSVs and associated URLs\n",
    "    return db_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa98a-c3ba-4108-98bd-5a0b4af0cd17",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Get database to csv** with **get_database_to_csv()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62c742-44a6-46c0-bfca-d5fece235ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Oxford Master data time series URL\n",
    "url_root_oxford = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/'\n",
    "\n",
    "#### List of CSVs of Oxford database Feel free to add more feature...\n",
    "## index_strigency missed!\n",
    "## 'E3;Fiscal measures;' missed!\n",
    "## 'E4;International support;' missed!\n",
    "## 'H3;Contact tracing;' missed!\n",
    "## 'H4;Emergency investment in healthcare!\n",
    "## 'H5;Investment in vaccines missed!\n",
    "## 'V1;Vaccine Prioritisation missed!\n",
    "## 'V2;Vaccine Availability missed!\n",
    "## 'V3;Vaccine Financial Support!\n",
    "## 'V4;Mandatory Vaccination missed!\n",
    "csv_list = ['confirmed_cases.csv', 'confirmed_deaths.csv',\n",
    "            'government_response_index_avg.csv', 'stringency_index_avg.csv', \n",
    "            'containment_health_index_avg.csv', 'economic_support_index.csv',\n",
    "            'c1m_school_closing.csv', 'c2m_workplace_closing.csv',\n",
    "            'c3m_cancel_public_events.csv', 'c4m_restrictions_on_gatherings.csv', \n",
    "            'c5m_close_public_transport.csv', 'c6m_stay_at_home_requirements.csv',\n",
    "            'c7m_movementrestrictions.csv', 'c8ev_internationaltravel.csv',\n",
    "            'e1_income_support.csv', 'e2_debtrelief.csv',\n",
    "            'h1_public_information_campaigns.csv', 'h2_testing_policy.csv',\n",
    "            'h3_contact_tracing.csv', 'h6m_facial_coverings.csv',\n",
    "            'h7_vaccination_policy.csv', 'h8m_protection_of_elderly_ppl.csv'\n",
    "           ] ## ; print('csv_list', csv_list, 'len(csv_list)', len(csv_list))\n",
    "    \n",
    "### Vacinations Dataset URLs\n",
    "url_root_vaccinations = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/'\n",
    "\n",
    "#### List of CSVs of Vaccinations database\n",
    "csv_list_vax = ['vaccinations.csv', 'vaccinations-by-age-group.csv'] ## ; print('csv_list', csv_list_vax, 'len(csv_list)', len(csv_list_vax))\n",
    "\n",
    "### Create a database grid all CSVs and associated URLs from Oxford website\n",
    "db_grid = get_database_to_csv(url_root_oxford, csv_list, data_dir)\n",
    "### Insert into database grid all CSVs and associated URLs from vaccinations website\n",
    "db_grid = get_database_to_csv(url_root_vaccinations, csv_list_vax, data_dir, db_grid)\n",
    "# print('db_grid', db_grid)\n",
    "\n",
    "# Stack all csl in the list\n",
    "csv_list += csv_list_vax ## ; print('csv_list', csv_list)\n",
    "\n",
    "# transform list into dict:\n",
    "csv = dict(zip(csv_list, [v[0] for v in enumerate(csv_list)])) ## ; print ('csv', csv) ## if v[1] == 'containment_health_index_avg.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8b3bb-c12b-43a3-9dda-7bbd905f3622",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d5c4a-c489-4204-a782-08cfb9a127c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc67099-9098-47c4-9e1d-b2754e07cd70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Read CSV** and **Set raw dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d45ada",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Read raw CSV** and **Set dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_gov_response = pd.read_csv(data_dir + 'government_response_index_avg.csv')\n",
    "df_raw_health = pd.read_csv(data_dir + 'containment_health_index_avg.csv')\n",
    "df_raw_economic = pd.read_csv(data_dir + 'economic_support_index.csv')\n",
    "\n",
    "#### Vaccination\n",
    "df_raw_vaccination = pd.read_csv(data_dir + 'vaccinations.csv')\n",
    "df_raw_ages = pd.read_csv(data_dir + 'vaccinations-by-age-group.csv')\n",
    "\n",
    "\n",
    "#### Data Frame target\n",
    "df_raw_cases = pd.read_csv(data_dir + 'confirmed_cases.csv')\n",
    "df_raw_deaths = pd.read_csv(data_dir + 'confirmed_deaths.csv')\n",
    "\n",
    "#### Data multiple\n",
    "df_raw_school_closing=pd.read_csv(data_dir + 'c1m_school_closing.csv')\n",
    "df_raw_workplace_closing=pd.read_csv(data_dir + 'c2m_workplace_closing.csv')\n",
    "df_raw_cancel_public_event=pd.read_csv(data_dir + 'c3m_cancel_public_events.csv')\n",
    "df_raw_restriction_on_gathering=pd.read_csv(data_dir + 'c4m_restrictions_on_gatherings.csv')\n",
    "df_raw_stay_at_home=pd.read_csv(data_dir + 'c6m_stay_at_home_requirements.csv')\n",
    "df_raw_international_travel=pd.read_csv(data_dir + 'c6m_stay_at_home_requirements.csv')\n",
    "df_raw_goverment_response=pd.read_csv(data_dir + 'government_response_index_avg.csv')\n",
    "df_raw_facial_covering=pd.read_csv(data_dir + 'h6m_facial_coverings.csv')\n",
    "df_raw_vacination_policy=pd.read_csv(data_dir + 'h7_vaccination_policy.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f43d38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Read out CSV** and **Set dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647dd56-8eaf-4f56-b52f-173e818e245c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### CSV Data out project directory\n",
    "csv_dir = '../data/csv_out/'\n",
    "# ! unzip {csv_dir}usa_index\n",
    "# ! unzip {csv_dir}usa_indicator\n",
    "# ! rm ECG_data.zip\n",
    "df_usa_index =  pd.read_csv(csv_dir + 'usa_index.csv')\n",
    "df_usa_indicator =  pd.read_csv(csv_dir + 'usa_indicator.csv')\n",
    "df_usa_index, df_usa_indicator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaecf123-5e8d-4e1e-b337-e0164c78fa4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataFrames setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b163434-318c-4d4c-bd02-c28a81314375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa_alb_index.head(), df_usa_alb_indicator.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ed76f-6978-4e8d-84a6-41b4cd8cb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sumedha csv test\n",
    "df_ts_usa_index =  df_usa_index.copy()\n",
    "df_ts_usa_indicator =  df_usa_indicator.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780e433-b780-4736-9011-36e69a97910d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Time series analysing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b05ade-58d9-4909-85f7-270a32dc2480",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Time Series Analysis *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9490e09-78eb-4563-881a-ea7d9f40b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cases = df_raw_cases.drop(columns=['country_name','region_code','region_name','jurisdiction','Unnamed: 0'])\n",
    "ts_cases = ts_cases.groupby('country_code').agg('sum')\n",
    "ts_cases.transpose()\n",
    "ts_cases.columns.name = 'Dates'\n",
    "ts_cases = ts_cases.fillna(0)\n",
    "# ts_cases.index = pd.to_datetime(ts_cases.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9518a-a848-45c5-b0cf-a1dc2363b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cases = ts_cases.transpose()\n",
    "ts_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e0fb8-f976-4dbe-8243-7a69be38b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5ecb3-6362-430f-bdc8-f8f2298c120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_ts_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54395e-6a75-45b7-a389-7f739b5c35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_ts_cases = vn_ts_cases.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca627f6-23e4-4243-8a68-e0b3108b21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_cases.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75735fc5-e04d-46ae-9d96-92dccdc1aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get VietNam country dataset\n",
    "vn_data = df_raw_cases.loc[df_raw_cases['country_code'] == 'VNM'].copy()\n",
    "\n",
    "vn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f336df2-286a-4819-a011-c1ae0d7cdb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vn_data.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122911e-70c8-4131-bf2f-8db270b3ac93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TENSORFLOW & RNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636afbb-aab0-4500-897b-311961289203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recurrent Neural Network (sequences data) modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f081b-7ee6-405b-8e13-04bf267ac2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Samples/Sequences, Observations, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6427dc3-25b3-4a5a-abba-20e92d67ab1b",
   "metadata": {},
   "source": [
    "X.shape = (n_SEQUENCES, n_OBSERVATIONS, n_FEATURES)\n",
    "\n",
    "y = RNN(X)\n",
    "\n",
    "‚ùóÔ∏è Notation $X_{i,j}^{t}$\n",
    "\n",
    " $_{i}$ is the sample/sequence\n",
    " \n",
    " $_{j}$ is the feature measured\n",
    " \n",
    " $^{t}$ is the time at which the observation is seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661159e6-4eb3-48cc-b11d-fff40fb81a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alberto train set\n",
    "# 3. Training\n",
    "def train_rnn_model(model, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 1,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "# print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8151dc-1f0e-4981-91a0-c1164230af26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "- **retrieve dataset** from Sumedha & Alberto\n",
    "\n",
    "    - **clean dataset**: \n",
    "        \n",
    "        - **drop first lines == 0** *(before Covid arrived)*\n",
    "        \n",
    "        - **check Nan**: \n",
    "- **strategy 1 country by country** sequences split as follow:\n",
    "\n",
    "- **strategy 2 one sequence per country**:\n",
    "    - **split X train, set** \n",
    "    - **Pad sequences**\n",
    "    - **create one csv per country**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32848eaa-a889-47ee-972c-e9f2e54214cc",
   "metadata": {},
   "source": [
    "## Training strategies:\n",
    "- Get NB dataset (cleaned) from Alberto & Sumedha\n",
    "- 1/ Indicator in precentage %\n",
    "- 2/ Indicator as categorical labels\n",
    "- Run same RNN model in parallel with Kim & Thomas\n",
    "- Identify best dataset\n",
    "- Parameters to fit:\n",
    "    - increase **nb of sequences**\n",
    "    - train series modulation (ex: [50, 150, 200, 300, 400 nb of days = n_obs]) < take time to compute\n",
    "    - **learning_rate** in Optimizer(parameters)\n",
    "    - model layers architecture (**simple** -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "        > LSTM\n",
    "        > Dense\n",
    "       (> LSTM\n",
    "        > LSTM\n",
    "        > Dense)\n",
    "     >> **try to overfit** the model with the loss (train over val) or (early_stopping)\n",
    "     >> **(X_val, y_val)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4cdbb-56aa-4522-84a6-833471a7b34a",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ee15a-c507-4049-9489-1f546a1f0a8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Series **Analysis** & **Preparation** to training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245d337-90d7-47b1-a4ee-b508e8731c39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test datasplit the dataset into training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe70c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alberto train set\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('United States', split_train=0.7, split_val=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe70c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/t9cq46wj2zl48zr3_0lc0rw80000gn/T/ipykernel_7176/3223474587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             X_len=n_obs, y_len=n_pred, n_sequences=n_seq)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m X_test, y_test = get_X_y_2(X_test, y_test, \n\u001b[0m\u001b[1;32m      6\u001b[0m                            X_len=n_obs, y_len=n_pred, n_sequences=n_seq_test)\n\u001b[1;32m      7\u001b[0m X_val, y_val = get_X_y_2(X_val, y_val, \n",
      "\u001b[0;32m/var/folders/hz/t9cq46wj2zl48zr3_0lc0rw80000gn/T/ipykernel_7176/2930949208.py\u001b[0m in \u001b[0;36mget_X_y_2\u001b[0;34m(X, y, X_len, y_len, n_sequences)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubsample_sequence_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mX_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hz/t9cq46wj2zl48zr3_0lc0rw80000gn/T/ipykernel_7176/1809187666.py\u001b[0m in \u001b[0;36msubsample_sequence_2\u001b[0;34m(X, y, X_len, y_len)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlast_possible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# How to split sequences? we could do it manually...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrandom_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_possible\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# X start and y end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_start\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mrandom_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "# Alberto train set\n",
    "X_train, y_train = get_X_y_2(X_train, y_train, \n",
    "                            X_len=n_obs, y_len=n_pred, n_sequences=n_seq)\n",
    "\n",
    "X_test, y_test = get_X_y_2(X_test, y_test, \n",
    "                           X_len=n_obs, y_len=n_pred, n_sequences=n_seq_test)\n",
    "X_val, y_val = get_X_y_2(X_val, y_val, \n",
    "                         X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "\n",
    "# np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922fc086-caa8-4117-ada2-60e8f5d85d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alberto train set\n",
    "n_seq = 200 ## nb of sequences (samples)\n",
    "n_obs = 150 # 150 days of training period (observations)\n",
    "n_feat = X_train.shape[1] # 20 feature:\n",
    "n_pred = 1 # nb of days where we can predict new daily deaths\n",
    "n_seq_val = n_seq // 7 # number of sequences in test set ?\n",
    "n_seq_test = n_seq // 10 # number of sequences in test set ?\n",
    "n_seq_test, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12d291e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ts_usa_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/t9cq46wj2zl48zr3_0lc0rw80000gn/T/ipykernel_7176/3569937617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;31m# ts_cases.shape[0] - 1 # nb of countries (samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m \u001b[0;31m# 15 days of training period (observations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mn_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_ts_usa_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 1 # 1feature: covid cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mn_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# nb of days of prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# length of a sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ts_usa_index' is not defined"
     ]
    }
   ],
   "source": [
    "n_seq = 200 # ts_cases.shape[0] - 1 # nb of countries (samples)\n",
    "n_obs = 150 # 15 days of training period (observations)\n",
    "n_feat = df_ts_usa_index.shape[1] # 1 # 1feature: covid cases\n",
    "n_pred = 1 # nb of days of prediction\n",
    "# length of a sequence\n",
    "len_seq = n_obs + n_pred\n",
    "# Ex: 16 = 15 + 1\n",
    "len_seq, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46650e9f-0186-4b2f-a981-70723ebc9f1c",
   "metadata": {},
   "source": [
    "Let's not cross-validate in this challenge to start with ü§Ø \n",
    "- Separate `df` into `df_train` and `df_test` such that the first 80% of the dataframe is in the training, and the last 20% in the test set.\n",
    "- Then generate (`X_train`, `y_train`) from `df_train` and (`X_test`, `y_test`) from `df_test`\n",
    "- Ensure that `X_train.shape == (50, 15, 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f30c3-7747-471f-b64c-f9ac6854f6ad",
   "metadata": {},
   "source": [
    "len_ = int(0.8*ts_cases.shape[0])\n",
    "df_train = ts_cases[:len_] ; df_test = ts_cases[len_:]\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c4c4c-7de5-4927-8b93-8e40cc8784da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(df_train, n_seq, len_seq, feature=y_seq)\n",
    "X_test, y_test = get_X_y(df_test, n_seq_test, len_seq, feature=y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c4c4c-7de5-4927-8b93-8e40cc8784da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(df_train, n_seq, len_seq, feature=y_seq)\n",
    "X_test, y_test = get_X_y(df_test, n_seq_test, len_seq, feature=y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd4799-c81a-4505-b9ed-4978b6258ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4fba2-9562-49eb-9b94-7c5ab57fd163",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create sequences (`X`,`y`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e6437-7538-4c29-b8d9-d00d4d804064",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Generates an entire dataset of multiple subsamples with shape $(X, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188b112-9ee9-4cb9-94d8-43724f36a493",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **subsample_sequence(df, length)**\n",
    "\n",
    "function that given a full dataframe `df`:\n",
    "- Create a sub sequences df, with long length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e91541",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0, 259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f29b8f8-53bc-4bb0-a509-b7b2e1f3321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_sequence_2(X, y, X_len, y_len) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given the initial dataframe `df`, return a shorter dataframe sequence of length `length` (eg n_obs).\n",
    "    This shorter sequence should be selected at random\n",
    "    \"\"\"\n",
    "    last_possible = X.shape[0] - X_len - y_len\n",
    "    # How to split sequences? we could do it manually...\n",
    "    random_start = np.random.randint(0, last_possible)\n",
    "    # X start and y end\n",
    "    X_sample = X[random_start : random_start + X_len]\n",
    "    y_sample = y[random_start : random_start + X_len + y_len]\n",
    "    \n",
    "    return np.array(X_sample), np.array(y_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29b8f8-53bc-4bb0-a509-b7b2e1f3321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_sequence(df, length) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given the initial dataframe `df`, return a shorter dataframe sequence of length `length` (eg n_obs).\n",
    "    This shorter sequence should be selected at random\n",
    "    \"\"\"\n",
    "    last_possible = df.shape[0] - length\n",
    "    # How to split sequences? we could do it manually...\n",
    "    random_start = np.random.randint(0, last_possible)\n",
    "    df_sample = df[random_start: random_start+length]\n",
    "    \n",
    "    return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb7159-4882-4d1a-a579-793c570bede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it \n",
    "# assert subsample_sequence(vn_ts_cases, 10).shape  == (10, 1)\n",
    "# assert subsample_sequence(vn_ts_cases, 400).shape == (400, 1)\n",
    "subsample_sequence(df_ts_usa_index, 10).shape, subsample_sequence(df_ts_usa_index, 400).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f77ac8-0106-4c19-9e8a-0f25e985a94d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **split_subsample_sequence(df,  length, sequence='VNM', df_mean=None)**\n",
    "\n",
    "function that given a full dataframe `df`:\n",
    "- Create a sub sequences df\n",
    "- Stores the value of the covid deaths* (or cases) of the last day as your variable array `y`\n",
    "- Stores all features of previous days as a variable `X`\n",
    "- Returns (`X`, `y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d7777-02cf-4205-aec4-69179c182de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subsample_sequence_2(X, y,  length, y_size=1) -> tuple:\n",
    "    '''\n",
    "    Create one single random (X_sample, y_sample)\n",
    "    containing one sequence each of length `length`\n",
    "    ToDo: Adapt the y size=-1'''\n",
    "    # Trick to save time during potential recursive calls\n",
    "    # if df_mean is None:\n",
    "    #     df_mean = df.mean()\n",
    "    X_subsample, y_subsample = subsample_sequence_2(X, y, X_len=n_obs, y_len=n_pred)\n",
    "   \n",
    "\n",
    "    return np.array(X_sample), np.array(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d7777-02cf-4205-aec4-69179c182de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subsample_sequence(df,  length, feature='VNM', y_size=1) -> tuple:\n",
    "    '''\n",
    "    Create one single random (X_sample, y_sample)\n",
    "    containing one sequence each of length `length`\n",
    "    ToDo: Adapt the y size=-1'''\n",
    "    # Trick to save time during potential recursive calls\n",
    "    # if df_mean is None:\n",
    "    #     df_mean = df.mean()\n",
    "    df_subsample = subsample_sequence(df, length)\n",
    "    y_sample = df_subsample.iloc[length - y_size][feature] # ['VNM'] ['VNM_covid_cases', 'VNM_covd_deaths'] \n",
    "    # Case y_sample is NaN: redraw !\n",
    "    # if y_sample != y_sample: # A value is not equal to itself only for NaN\n",
    "    #         X_sample, y_sample = split_subsample_sequence(df, length, df_mean) # Recursive call !!!\n",
    "    #         return np.array(X_sample), np.array(y_sample)    \n",
    "    X_sample = df_subsample[0:length - y_size]\n",
    "    # Case X_sample has some NaNs\n",
    "    # if X_sample.isna().sum().sum() !=0:\n",
    "    #    X_sample = X_sample.fillna(compute_means(X_sample, df_mean))\n",
    "    #    X_sample = X_sample.values\n",
    "\n",
    "    return np.array(X_sample), np.array(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993c868-606c-4d31-ac8d-997bc85d63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "print(y_seq)\n",
    "(X_sample, y_sample) = split_subsample_sequence(df_ts_usa_index, feature=y_seq, length=len_seq, y_size=n_pred)\n",
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb0927-e52b-4d41-8dcb-6ca6f204a1f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **get_X_y(df, n_sequences, length)**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0c33ac-32c7-45ea-b554-e37f91a87701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_2(X, y, X_len, y_len, n_sequences) -> tuple:\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        (xi, yi) = subsample_sequence_2(X, y, X_len=X_len, y_len=y_len)\n",
    "        X_list.append(xi)\n",
    "        y_list.append(yi)\n",
    "        \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c33ac-32c7-45ea-b554-e37f91a87701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(df, n_sequences, length, feature='VNM') -> tuple:\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        (xi, yi) = split_subsample_sequence(df, length, feature=feature)\n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4d481-caeb-4f65-90cf-44c386e9a4cb",
   "metadata": {},
   "source": [
    "Generate your dataset $(X, y)$ of `50` sequences, each of `15` observations + the covid cases at the 16-th day to predict\n",
    "\n",
    "n_seq = 50 # ts_cases.shape[0] - 1 # nb of countries (samples)\n",
    "n_obs = 15 # 15 days of training periiod (observations)\n",
    "n_feat = 1 # 1feature: covid cases\n",
    "n_pred = 1 # nb of days of prediction\n",
    "len_seq = 16 # length of a sequence (len_seq = n_obs + n_pred/ Ex: 16 = 15 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c5f66-370f-4fec-a25b-c923775b025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y(df_ts_usa_index, n_sequences=n_seq, length=len_seq, feature=y_seq)\n",
    "print(X.shape) ; print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d625ec-6a3a-4e5a-b425-d0332f9a789c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Test it\n",
    "(X_sample, y_sample) = split_subsample_sequence(vn_ts_cases, length=len_seq, y_size=n_pred)\n",
    "###### assert X_sample.shape == (n_obs,n_feat)\n",
    "###### assert y_sample.shape == ()\n",
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756c13e-9a2c-4e81-aa17-55c390dd4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your code below\n",
    "assert X.shape == (50, 15, 1)\n",
    "assert y.shape == (50, )\n",
    "assert np.isnan(X).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe229ed-38be-485e-9314-5a3c04da8847",
   "metadata": {},
   "source": [
    "### How to split sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49128bf-f5e8-4319-8635-fa1f63c29ceb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- randomly or\n",
    "\n",
    "- manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d29d30-5758-4941-b217-ccefe2116237",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Samples/Sequences, Observations, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad867c99-9dd9-4dc5-9cda-de741bf57ca2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **train_rnn_model(model, patience=2, epochs=200):**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3557ab-55e0-4ae2-a289-212d608dc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(X_val=0, y_val=0):\n",
    "    [print(f'validation_data=(X_val, y_val),') if (X_val!=0 or y_val!=0) else print(f'validation_split=0.1,')]\n",
    "    return True\n",
    "\n",
    "train_rnn(),train_rnn((1),(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc24c32-f615-4696-b4e0-01e7d7281ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model(model, patience=20, epochs=200, X_val=0, y_val=0):\n",
    "    \"\"\" function that train a RNN model with hyperparameters:\n",
    "    - patience by default 2 to early stop\n",
    "    - epochs by default 200 to train over several epochs\n",
    "    - valisation data by default (X_val, y_val)=(0, 0) in case of auto split\n",
    "    \"\"\"\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train,\n",
    "            y_train, \n",
    "             # Auto split for validation data\n",
    "            [print(f'validation_data=(X_val, y_val),') if (X_val!=0 or y_val!=0) else print(f'validation_split=0.1,')]\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884eb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model(model, patience=20, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train,\n",
    "            y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ece011-197d-439e-b405-ee2370c9773d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c397c28d-f9f1-4d6c-9ed2-17a617e42352",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alberto model #4 test \n",
    "# Normalization layer not necessary as X_train already train\n",
    "# normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "# normalizer.adapt(X_train) # \"Fit\" it on the train set\n",
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_4 = Sequential()\n",
    "# rnn_model_4.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_4.add(LSTM(units=n_feat, activation='relu'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_4.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_4.add(Dense(n_pred, activation = 'linear'))\n",
    "#ValueError: Input 0 of layer \"lstm_1\" is incompatible with the layer:\n",
    "#     >>> expected ndim=3, found ndim=2. Full shape received: (None, 20)#\n",
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_4.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_4.summary()\n",
    "\n",
    "# 3. Training\n",
    "history = train_rnn_model(model=rnn_model_4, epochs=200, patience=3)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "\n",
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50f5c4-faa0-4355-a1fd-a28b28062623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. The Normalization Layer\n",
    "normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model #3 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e901218-eb1c-4d85-b40e-9beda6c3c548",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model #1 evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1267f1-8a86-4f8b-82f5-01478bc0d350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c615d0-d83c-4b7d-b076-b914179fb0ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Series Forecasting with model #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model #3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a685f57-01a6-4405-8b38-0cc3c7fa5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571254c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_rnn_model(rnn_model, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trai### Train model #1n_series = [50, 150, 200, 300, 400]\n",
    "overfit_es =   [2, 6, 6, 5, 6 ]\n",
    "print('type(overfit_es), overfit_es', type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "for i in range(len(train_series)):\n",
    "    history = train_rnn_model(model=rnn_model_2, epochs=train_series[i], patience=overfit_es[i])\n",
    "    plt.plot(history.history['mape'])\n",
    "    plt.plot(history.history['val_mape'])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1950356",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model #3 architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b325a69",
   "metadata": {},
   "source": [
    "#### üöÄ The **LSTM (= Long Short Term Memory)** with their ability to *avoid the vanishing gradient problem*, should be preferred over a SimpleRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae1226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model #3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f878a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1470a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model #3 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model #3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cc51a-a8a3-4a61-9c9e-96317f159d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecee49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_3, patience=2, epochs=200, (X_val, y_val)=(0, 0)):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_3.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min(history.history['mape'])\n",
    "\n",
    "# print(\"adjust early stopping\")\n",
    "# overfit_es = [d[0]+1 for d in enumerate(history.history['mape']) if d[1] == min(history.history['mape'])][0]\n",
    "# overfit_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43e359-c428-43f9-9369-f0804a5acdac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min(history.history['mape']), max(history.history['mape']), history.history['mape'] # blue line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f6eb0-e734-48c4-a455-49949ea62a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(history.history['val_mape']), history.history['val_mape'] # orange line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba3da5-6613-418e-9919-2f7d50e0078e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model #1 evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed1ac5-3b8a-4049-abe8-9431de2121c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32232f29-fc87-49fc-9f6f-11dcb25a9f92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Time Series Forecasting with model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a68310-d512-4825-a2c4-21cb68e5f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your code below\n",
    "assert y_pred.shape == (n_seq_test, n_pred)\n",
    "# Distribution of the real values y_train\n",
    "pd.DataFrame(y_train).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cad73-73e8-4211-908a-3789b7a03497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the real values y_train\n",
    "pd.DataFrame(y_train).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trai### Train model #1n_series = [50, 150, 200, 300, 400]\n",
    "overfit_es =   [2, 6, 6, 5, 6 ]\n",
    "print('type(overfit_es), overfit_es', type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "for i in range(len(train_series)):\n",
    "    history = train_rnn_model(model=rnn_model_2, epochs=train_series[i], patience=overfit_es[i])\n",
    "    plt.plot(history.history['mape'])\n",
    "    plt.plot(history.history['val_mape'])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f576952-8105-43b3-9d97-39d19277c207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6cdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "9c7d4b3da1e137aeabc4fd6b55dede4bb1c85dbdc50880a4fde3d2604903e3dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
